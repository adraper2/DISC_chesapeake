counter <- 1
for(x in 1:length(reviews)){
reviews <- gsub("Pros", " ", reviews)
reviews <- gsub("Cons", " ", reviews)
current <- substr(x=reviews[x],start=1, stop = regexpr('Share on Facebook', reviews[x])[1]-1)
review_data[counter] <- current
counter <- counter + 1
print(paste(current))
}
}
return(review_data)
}
}
review_data <- getReviews(url="https://www.glassdoor.com/Reviews/U-S-Bank-Reviews-E8937.htm",page_num = 430)
review_data
con<-file('~/Documents/Junior_Year/DataAnalytics/ManhattanUS_bank.csv',encoding="UTF-8")
write.csv(review_data, file = con)
getReviews <- function(url="https://www.glassdoor.com/Reviews/Veolia-Reviews-E20114.htm", page_num=1){
if (page_num <= 1){
webpage <- url %>% read_html()
reviews <- webpage %>% html_nodes( ".reviewBodyCell") %>% html_text()
review_data <- vector(mode = "character", length = length(reviews))
for(x in 1:length(reviews)){
reviews <- gsub("Pros", " ", reviews)
reviews <- gsub("Cons", " ", reviews)
current <- substr(x=reviews[x],start=1, stop = regexpr('Share on Facebook', reviews[x])[1]-1)
review_data[x] <- current
print(paste(current))
}
return(review_data)
} else {
url_list <- vector(mode = "character", length = page_num)
for (i in 1:page_num){
if(i>1){
url_list[i] <- paste(substr(x=url,start=1,stop=(nchar(url)-4)), "_P",as.character(i),".htm", sep="")
} else{
url_list[i] <- url
}
}
for (y in 1:page_num){
url <- url_list[y]
webpage <- url %>% read_html()
reviews <- webpage %>% html_nodes( ".reviewBodyCell") %>% html_text()
review_data <- vector(mode = "character", length = page_num*10)
counter <- 1
for(x in 1:length(reviews)){
reviews <- gsub("Pros", " ", reviews)
reviews <- gsub("Cons", " ", reviews)
current <- substr(x=reviews[x],start=1, stop = regexpr('Share on Facebook', reviews[x])[1]-1)
review_data[counter] <- current
counter <- counter + 1
#print(paste(current))
}
}
return(review_data)
}
}
review_data <- getReviews(url="https://www.glassdoor.com/Reviews/U-S-Bank-Reviews-E8937.htm",page_num = 20)
con<-file('~/Documents/Junior_Year/DataAnalytics/Manhattan/US_bank.csv',encoding="UTF-8")
write.csv(review_data, file = con)
library(rvest)
getReviews <- function(url="https://www.glassdoor.com/Reviews/Veolia-Reviews-E20114.htm", page_num=1){
if (page_num <= 1){
webpage <- url %>% read_html()
reviews <- webpage %>% html_nodes( ".reviewBodyCell") %>% html_text()
review_data <- vector(mode = "character", length = length(reviews))
for(x in 1:length(reviews)){
reviews <- gsub("Pros", " ", reviews)
reviews <- gsub("Cons", " ", reviews)
current <- substr(x=reviews[x],start=1, stop = regexpr('Share on Facebook', reviews[x])[1]-1)
review_data[x] <- current
print(paste(current))
}
return(review_data)
} else {
url_list <- vector(mode = "character", length = page_num)
for (i in 1:page_num){
if(i>1){
url_list[i] <- paste(substr(x=url,start=1,stop=(nchar(url)-4)), "_P",as.character(i),".htm", sep="")
} else{
url_list[i] <- url
}
}
counter <- 1
for (y in 1:page_num){
url <- url_list[y]
webpage <- url %>% read_html()
reviews <- webpage %>% html_nodes( ".reviewBodyCell") %>% html_text()
review_data <- vector(mode = "character", length = page_num*10)
for(x in 1:length(reviews)){
reviews <- gsub("Pros", " ", reviews)
reviews <- gsub("Cons", " ", reviews)
current <- substr(x=reviews[x],start=1, stop = regexpr('Share on Facebook', reviews[x])[1]-1)
review_data[counter] <- current
counter <- counter + 1
#print(paste(current))
}
}
return(review_data)
}
}
review_data <- getReviews(url="https://www.glassdoor.com/Reviews/U-S-Bank-Reviews-E8937.htm",page_num = 20)
con<-file('~/Documents/Junior_Year/DataAnalytics/Manhattan/US_bank.csv',encoding="UTF-8")
write.csv(review_data, file = con)
reviews <- webpage %>% html_nodes( ".reviewBodyCell") %>% html_text()
library(rvest)
getReviews <- function(url="https://www.glassdoor.com/Reviews/Veolia-Reviews-E20114.htm", page_num=1){
if (page_num <= 1){
webpage <- url %>% read_html()
reviews <- webpage %>% html_nodes( ".reviewBodyCell") %>% html_text()
review_data <- vector(mode = "character", length = length(reviews))
for(x in 1:length(reviews)){
reviews <- gsub("Pros", " ", reviews)
reviews <- gsub("Cons", " ", reviews)
current <- substr(x=reviews[x],start=1, stop = regexpr('Share on Facebook', reviews[x])[1]-1)
review_data[x] <- current
print(paste(current))
}
return(review_data)
} else {
url_list <- vector(mode = "character", length = page_num)
for (i in 1:page_num){
if(i>1){
url_list[i] <- paste(substr(x=url,start=1,stop=(nchar(url)-4)), "_P",as.character(i),".htm", sep="")
} else{
url_list[i] <- url
}
}
counter <- 1
review_data <- vector(mode = "character", length = page_num*10)
for (y in 1:page_num){
url <- url_list[y]
webpage <- url %>% read_html()
reviews <- webpage %>% html_nodes( ".reviewBodyCell") %>% html_text()
for(x in 1:length(reviews)){
reviews <- gsub("Pros", " ", reviews)
reviews <- gsub("Cons", " ", reviews)
current <- substr(x=reviews[x],start=1, stop = regexpr('Share on Facebook', reviews[x])[1]-1)
review_data[counter] <- current
counter <- counter + 1
#print(paste(current))
}
}
return(review_data)
}
}
review_data <- getReviews(url="https://www.glassdoor.com/Reviews/U-S-Bank-Reviews-E8937.htm",page_num = 20)
con<-file('~/Documents/Junior_Year/DataAnalytics/Manhattan/US_bank.csv',encoding="UTF-8")
write.csv(review_data, file = con)
library(rvest)
getReviews <- function(url="https://www.glassdoor.com/Reviews/Veolia-Reviews-E20114.htm", page_num=1){
if (page_num <= 1){
webpage <- url %>% read_html()
reviews <- webpage %>% html_nodes( ".reviewBodyCell") %>% html_text()
review_data <- vector(mode = "character", length = length(reviews))
for(x in 1:length(reviews)){
reviews <- gsub("Pros", " ", reviews)
reviews <- gsub("Cons", " ", reviews)
current <- substr(x=reviews[x],start=1, stop = regexpr('Share on Facebook', reviews[x])[1]-1)
review_data[x] <- current
print(paste(current))
}
return(review_data)
} else {
url_list <- vector(mode = "character", length = page_num)
for (i in 1:page_num){
if(i>1){
url_list[i] <- paste(substr(x=url,start=1,stop=(nchar(url)-4)), "_P",as.character(i),".htm", sep="")
} else{
url_list[i] <- url
}
}
counter <- 1
review_data <- vector(mode = "character", length = page_num*10)
for (y in 1:page_num){
url <- url_list[y]
webpage <- url %>% read_html()
reviews <- webpage %>% html_nodes( ".reviewBodyCell") %>% html_text()
for(x in 1:length(reviews)){
reviews <- gsub("Pros", " ", reviews)
reviews <- gsub("Cons", " ", reviews)
reviews <- gsub("Advice to Management", " ", reviews)
current <- substr(x=reviews[x],start=1, stop = regexpr('Share on Facebook', reviews[x])[1]-1)
review_data[counter] <- current
counter <- counter + 1
#print(paste(current))
}
}
return(review_data)
}
}
review_data <- getReviews(url="https://www.glassdoor.com/Reviews/U-S-Bank-Reviews-E8937.htm",page_num = 430)
con<-file('~/Documents/Junior_Year/DataAnalytics/Manhattan/US_bank.csv',encoding="UTF-8")
write.csv(review_data, file = con)
review_data <- getReviews(url="https://www.glassdoor.com/Reviews/Transdev-Reviews-E413452.htm",page_num = 24)
con<-file('~/Documents/Junior_Year/DataAnalytics/Manhattan/Transdev.csv',encoding="UTF-8")
write.csv(review_data, file = con)
review_data <- getReviews(url="https://www.glassdoor.com/Reviews/First-Student-Reviews-E16694.htm",page_num = 44)
review_data <- getReviews(url="https://www.glassdoor.com/Reviews/First-Student-Reviews-E16694.htm",page_num = 44)
con<-file('~/Documents/Junior_Year/DataAnalytics/Manhattan/First_student.csv',encoding="UTF-8")
write.csv(review_data, file = con)
review_data <- getReviews(url="https://www.glassdoor.com/Reviews/Veolia-Reviews-E20114.htm",page_num = 63)
con<-file('~/Documents/Junior_Year/DataAnalytics/Manhattan/Veolia.csv',encoding="UTF-8")
write.csv(review_data, file = con)
review_data <- getReviews(url="https://www.glassdoor.com/Reviews/Harvard-Pilgrim-Reviews-E2816.htm",page_num = 10)
con<-file('~/Documents/Junior_Year/DataAnalytics/Manhattan/Harvard_pilgrim.csv',encoding="UTF-8")
write.csv(review_data, file = con)
review_data <- getReviews(url="https://www.glassdoor.com/Reviews/TD-Reviews-E3767.htm",page_num = 144)
con<-file('~/Documents/Junior_Year/DataAnalytics/Manhattan/TD_bank.csv',encoding="UTF-8")
write.csv(review_data, file = con)
review_data <- getReviews(url="https://www.glassdoor.com/Reviews/Motorola-Solutions-Reviews-E427189.htm",page_num = 68)
con<-file('~/Documents/Junior_Year/DataAnalytics/Manhattan/Motorola.csv',encoding="UTF-8")
write.csv(review_data, file = con)
install.packages("gjam")
library(gjam)
install.packages("gjam")
install.packages("RcppArmadillo")
install.packages(gjam)
install.packages("gjam")
install.packages("RcppArmadillo")
install.packages("gjam")
library(gjam)
install.packages("coda")
install.packages("gridExtra")
install.packages('mgcv')
install.packages('zoo')
install.packages('Matrix')
install.packages('cowplot')
install.packages('spBayes')
install.packages('snow')
install.packages('snowfall')
install.packages('rlecuyer')
range(3,1)
diff(3,1)
25-21-1
25-21+1
l <- vector("list", 3)
l[[1]] <- 7
matrix(seq(1,6), seq(1,6)
)
matrix(seq(1,6), seq(1,6)
matrix(seq(1,6), seq(1,6))
data.frame(seq(1,6), seq(1,6))
knitr::opts_chunk$set(echo = TRUE)
choose(11,11)
install.packages("combinat")
permn(c("M","I","S","S","I","S","S","I", "P","P", "I"))
library(combinat)
permn(c("M","I","S","S","I","S","S","I", "P","P", "I"))
nrows??
q
??nrows
nrow(permn(c("M","I","S","S","I","S","S","I", "P","P", "I")))
#library(combinat)
2^11
total <- factorial(11)
total
ans2 <- (8)*(10^6)
(three.of.suit**3)*four.of.suit
total.hands <- 2**52
three.of.suit <- choose(13,3)
four.of.suit <- choose(13,4)
num.suits <- 4
ans33 <- (num.suits*(three.of.suit**3)*four.of.suit)/total.hands
cat("Probability is:", ans33)
pf()
pf(3,4)
pf(df1=3,df2=4)
pf(q=17.47,df1=3,df2=4)
1-pf(q=17.47,df1=3,df2=4)
65.5^2 * 4
8^2*4
4.5^2*2+3.5^2*2
256/32.5
knitr::opts_chunk$set(echo = TRUE)
choose(30, 5)
choose(25, 5)
choose(20, 5)
num.scenario <- choose(30, 5) * choose(25, 5) * choose(20, 5) * choose(15, 5) * choose(10, 5) * choose(5,5)
6!
factorial(6!)
factorial(6)
total.hands <- choose(52,13)
total.hands
7*(9^2)*10^4
ans2a <- (8)*(10^6)
ans2a - 10^4
choose(6^6, 6)
all.six <- choose(6^rolls, 6)
rolls <- 6 # some arbitrary number
total.rolls <- 6^rolls
all.six <- factorial(6)/total.rolls
(1 - all.six)
(6*((5/6)^rolls)) - (choose(6,2)*((4/6)^rolls)) + (choose(6,3)*((3/6)^rolls)) - (choose(6,4)*((2/6)^rolls)) + (choose(6,5)*((1/6)^rolls))
1 - factorial(rolls)/6^rolls
rolls <- 100 # some arbitrary number
paste(ans47,sep = ", "
paste(ans47,sep = ", ")
ans47 <- (6*one.num) - (choose(6,2)*two.num) + (choose(6,3)*three.num) - (choose(6,4)*four.num) + (choose(6,5)*five.num)
rolls <- 1:10 # some arbitrary number
#1 - factorial(rolls)/(6^rolls)
#1 - 6*choose(7,2)*factorial(5)/6^rolls
# The first works for six rolls but nothing else. The next only works for 7. I cannot do the inverse probability...
#the likelihood of seeing (n - numerator) many different numbers based on rolls
one.num <- (5/6)^rolls
two.num <- (4/6)^rolls
three.num <- (3/6)^rolls
four.num <- (2/6)^rolls
five.num <- (1/6)^rolls
# the final equation with the possibilities of different dice numbers available per roll as a multiplicative of the sums of all the different events
ans47 <- (6*one.num) - (choose(6,2)*two.num) + (choose(6,3)*three.num) - (choose(6,4)*four.num) + (choose(6,5)*five.num)
paste(ans47,sep = ", ")
print(ans47,sep = ", ")
paste0(ans47,collapse = ", ")
cat("Probabilities are:", paste0(ans47,collapse = ", "))
2^7
heads <- 1:10
formula <- choose(10,heads)/(2**10)
formula
sum(formula)
plot(formula)
formula[3]
sum(formula where heads = 6:10)
heads2 <- 6:10
sum(choose(10,heads2)/(2**10))
rast.noisy <- raster('images/IMG0722Noisy.bmp')
library(raster)
rast.noisy <- raster('images/IMG0722Noisy.bmp')
k <- 1:100
pmf <- choose(100,k) * .75**k * (1-.75)**(100-k)
hist(pmf)
plot(pmf)
hist(pmf)
plot(pmf)
coffee <- read.csv("~/Downloads/4 - coffee trees (1).txt", sep="")
View(coffee)
View(coffee)
plot(coffee, by=location)
plot(coffee, by=coffee$location)
plot(coffee$yield, by=coffee$location)
plot(coffee$yield, by=coffee$location)
library(ggplot2)
ggplot(data=coffee, aes(x=location, y=yield))
ggplot(data=coffee, aes(x=as.factor(location), y=yield))
ggplot(data=coffee, aes(x=as.factor(location), y=yield)) + geom_point()
ggplot(data=coffee, aes(x=as.factor(density), y=yield)) + geom_point()
p1 <- ggplot(data=coffee, aes(x=as.factor(location), y=yield)) + geom_point()
p2 <- ggplot(data=coffee, aes(x=as.factor(density), y=yield)) + geom_point()
grid.arrange(p1,p2)
library(gridExtra)
grid.arrange(p1,p2)
mean(coffee$yield)
aggregate(coffee$yield~coffee$location,FUN=mean)
aggregate(coffee$yield~coffee$density,FUN=mean)
21 + block.effect + factor.effect
block.effect <- c(3,3,3,3,-4,-4,-4,-4,1,1,1,1)
factor.effect <- c(-2,-1,1,2,-2,-1,1,2,-2,-1,1,2)
21 + block.effect + factor.effect
coffee$yield - fit
fit <- 21 + block.effect + factor.effect
coffee$yield - fit
coffee$yield
factor.effect <- c(2,1,-1,-2,2,1,-1,-2,2,1,-1,-2)
fit <- 21 + block.effect + factor.effect
coffee$yield - fit
aggregate(coffee$yield~as.factor(coffee$location),FUN=mean)
aggregate(coffee$yield~as.factor(coffee$density),FUN=mean)
33^12
33^2 *12
(33^2) *12
13068+2232+432+632+494
2232/2
632/2
494/6
1116/82.333333
432/82.333333
316/82.333333
qf(2,6)
pf(2,6)
nf(2,6)
qf(.95, 2,6)
qf(.95, 1,6)
pf(13.55,2,6)
1-pf(13.55,2,6)
nsamp <- 50
long <- -runif(nsamp, min=65, max=130)
lat <- runif(nsamp, min=25, max=50)
data <- cbind(sample=1:nsamp,
latitude=round(lat,4),
longitude=round(long,4),
in.cont=c(rep(NA, nsamp)),
in.mile==c(rep(NA, nsamp)),
location=character(nsamp),
notes=character(nsamp))
write.csv(data, file="~/Desktop/roadless.csv")
data <- cbind(sample=1:nsamp,
latitude=round(lat,4),
longitude=round(long,4),
in.cont=c(rep(NA, nsamp)),
in.mile==c(rep(NA, nsamp)),
location=character(nsamp),
notes=character(nsamp))
data <- cbind(sample=1:nsamp,
latitude=round(lat,4),
longitude=round(long,4),
in.cont=c(rep(NA, nsamp)),
in.mile=c(rep(NA, nsamp)),
location=character(nsamp),
notes=character(nsamp))
write.csv(data, file="~/Desktop/roadless.csv")
ds = read.csv("~/Documents/Senior_Year/STS\ 213/roadless.csv")
ds = read.csv("~/Documents/Senior_Year/STS\ 213/roadless.csv")
table(ds$incontinent)
table(ds$location[ds$incontinent==0])
table(ds$location[ds$incontinent==1])
smallids <- subset(ds, incontinent==1)
rm(ds)
table(smallids$within1mile)
k <- 26
n <- 8
binom.test(k,n)
k <- 26
n <- 8 + 26
binom.test(k,n)
rm(list=ls())
library(raster)
library(magrittr)
library(rgdal)
library(ggplot2)
setwd("~/Documents/Junior_Year/DISC_REU/DISC_chesapeake/")
# create our crop region layer
e <- as(extent(365375, 366400, 4303600, 4304800), 'SpatialPolygons')
crs(e) <- "+proj=utm +zone=18"
# import bands, crop them to the SERC region and raster to a data frame
band2 <- "Landsat8/LC08_L1TP_015033_20160718_20170222_01_T1/LC08_L1TP_015033_20160718_20170222_01_T1_B2.TIF" %>% raster() %>% crop(y = e) %>% rasterToPoints()
band3 <- "Landsat8/LC08_L1TP_015033_20160718_20170222_01_T1/LC08_L1TP_015033_20160718_20170222_01_T1_B3.TIF" %>% raster() %>% crop(y = e) %>% rasterToPoints()
band4 <- "Landsat8/LC08_L1TP_015033_20160718_20170222_01_T1/LC08_L1TP_015033_20160718_20170222_01_T1_B4.TIF" %>% raster() %>% crop(y = e) %>% rasterToPoints()
band5 <- "Landsat8/LC08_L1TP_015033_20160718_20170222_01_T1/LC08_L1TP_015033_20160718_20170222_01_T1_B5.TIF" %>% raster() %>% crop(y = e) %>% rasterToPoints()
# covariates for model and dataset construction
evi.value <- 2.5 * ((band5[1,3] - band4[1,3]) / (((band4[1,3] * 6) + band5[1,3]) - ((7.5 * band2[1,3]) + 1)))
ndvi.value <- ((band5[,3] - band4[,3]) / (band5[,3] + band4[,3]))
ndwi.value <- (band3[,3] - band5[,3]) / (band3[,3] + band5[,3])
savi.value <- (1.5 * ((band5[,3] - band4[,3]) / (band5[,3] + band4[,3] + 1.5)))
full.data <- data.frame(x = band2[,1],
y = band2[,2],
band2 = band2[,3],
band3 = band3[,3],
band4 = band4[,3],
band5 = band5[,3],
ndvi = ndvi.value,
evi = evi.value,
ndwi = ndwi.value,
savi = savi.value)
# Species Map Import
species <- read.csv("~/Documents/Junior_Year/DISC_REU/DISC_chesapeake/DominantSpPerPlot.csv")
# remove plots with no location, if any
species <- species[!is.na(species$easting),]
species <- species[!is.na(species$northing),]
# convert UTM to lat, long
utm.coor.serc <- SpatialPoints(cbind(species$easting,species$northing),
proj4string=CRS("+proj=utm +zone=18"))
# Convert to lat/long
long.lat.coor.serc <- as.data.frame(spTransform(utm.coor.serc,CRS("+proj=longlat")))
species.map <- data.frame(species[2:9],
lat = long.lat.coor.serc$coords.x1,
lon = long.lat.coor.serc$coords.x2,
easting = utm.coor.serc$coords.x1,
northing = utm.coor.serc$coords.x2)
rm(species, long.lat.coor.serc, utm.coor.serc)
training <- data.frame(plot.id = numeric(521), easting = numeric(521), northing = numeric(521),
scam = numeric(521), ivfr = numeric(521), c4 = numeric(521), phau = numeric(521),
spcy = numeric(521), tyla = numeric(521), dead= numeric(521), bare_water = numeric(521),
band2 = numeric(521), band3 = numeric(521), band4 = numeric(521), band5 = numeric(521),
ndvi = numeric(521), evi = numeric(521), ndwi = numeric(521), savi = numeric(521))
count <- 0
for(i in 1:nrow(full.data)){
for (j in 1:nrow(species.map)){
if (species.map[j,11] - full.data[i,1] < 30 & species.map[j,11] - full.data[i,1] > 0 &
species.map[j,12] - full.data[i,2] < 30 & species.map[j,12] - full.data[i,2] > 0){
count = count + 1
training$plot.id[count] <- i
training[count,-c(1)] <- c(full.data[i,1:2],species.map[j,1:8], full.data[i,3:10])
}
}
}
paste("Yes count:", count)
nrow(training)
unique(training$northing)
unique(training$easting)
unique(training$northing,training$easting)
count(unique(training$northing,training$easting))
frequency(unique(training$northing,training$easting))
len(unique(training$northing,training$easting))
length(unique(training$northing,training$easting))
length(unique(cbind(training$northing,training$easting)))
cbind(training$northing,training$easting)
unique(as.data.frame(cbind(training$northing,training$easting)))
distinct(as.data.frame(cbind(training$northing,training$easting)))
library(dplyr)
distinct(as.data.frame(cbind(training$northing,training$easting)))
mode(as.data.frame(cbind(training$northing,training$easting)))
uniqv <- unique(v)
getmode <- function(v) {
uniqv <- unique(v)
uniqv[which.max(tabulate(match(v, uniqv)))]
}
getmode(as.data.frame(cbind(training$northing,training$easting)))
getmode <- function(v) {
uniqv <- distinct(v)
uniqv[which.max(tabulate(match(v, uniqv)))]
}
getmode(as.data.frame(cbind(training$northing,training$easting)))
