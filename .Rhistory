}
for (y in 1:page_num){
url <- url_list[y]
webpage <- url %>% read_html()
reviews <- webpage %>% html_nodes( ".reviewBodyCell") %>% html_text()
review_data <- vector(mode = "character", length = page_num*10)
counter <- 1
for(x in 1:length(reviews)){
reviews <- gsub("Pros", " ", reviews)
reviews <- gsub("Cons", " ", reviews)
current <- substr(x=reviews[x],start=1, stop = regexpr('Share on Facebook', reviews[x])[1]-1)
review_data[counter] <- current
counter <- counter + 1
#print(paste(current))
}
}
return(review_data)
}
}
review_data <- getReviews(url="https://www.glassdoor.com/Reviews/U-S-Bank-Reviews-E8937.htm",page_num = 20)
con<-file('~/Documents/Junior_Year/DataAnalytics/Manhattan/US_bank.csv',encoding="UTF-8")
write.csv(review_data, file = con)
library(rvest)
getReviews <- function(url="https://www.glassdoor.com/Reviews/Veolia-Reviews-E20114.htm", page_num=1){
if (page_num <= 1){
webpage <- url %>% read_html()
reviews <- webpage %>% html_nodes( ".reviewBodyCell") %>% html_text()
review_data <- vector(mode = "character", length = length(reviews))
for(x in 1:length(reviews)){
reviews <- gsub("Pros", " ", reviews)
reviews <- gsub("Cons", " ", reviews)
current <- substr(x=reviews[x],start=1, stop = regexpr('Share on Facebook', reviews[x])[1]-1)
review_data[x] <- current
print(paste(current))
}
return(review_data)
} else {
url_list <- vector(mode = "character", length = page_num)
for (i in 1:page_num){
if(i>1){
url_list[i] <- paste(substr(x=url,start=1,stop=(nchar(url)-4)), "_P",as.character(i),".htm", sep="")
} else{
url_list[i] <- url
}
}
counter <- 1
for (y in 1:page_num){
url <- url_list[y]
webpage <- url %>% read_html()
reviews <- webpage %>% html_nodes( ".reviewBodyCell") %>% html_text()
review_data <- vector(mode = "character", length = page_num*10)
for(x in 1:length(reviews)){
reviews <- gsub("Pros", " ", reviews)
reviews <- gsub("Cons", " ", reviews)
current <- substr(x=reviews[x],start=1, stop = regexpr('Share on Facebook', reviews[x])[1]-1)
review_data[counter] <- current
counter <- counter + 1
#print(paste(current))
}
}
return(review_data)
}
}
review_data <- getReviews(url="https://www.glassdoor.com/Reviews/U-S-Bank-Reviews-E8937.htm",page_num = 20)
con<-file('~/Documents/Junior_Year/DataAnalytics/Manhattan/US_bank.csv',encoding="UTF-8")
write.csv(review_data, file = con)
reviews <- webpage %>% html_nodes( ".reviewBodyCell") %>% html_text()
library(rvest)
getReviews <- function(url="https://www.glassdoor.com/Reviews/Veolia-Reviews-E20114.htm", page_num=1){
if (page_num <= 1){
webpage <- url %>% read_html()
reviews <- webpage %>% html_nodes( ".reviewBodyCell") %>% html_text()
review_data <- vector(mode = "character", length = length(reviews))
for(x in 1:length(reviews)){
reviews <- gsub("Pros", " ", reviews)
reviews <- gsub("Cons", " ", reviews)
current <- substr(x=reviews[x],start=1, stop = regexpr('Share on Facebook', reviews[x])[1]-1)
review_data[x] <- current
print(paste(current))
}
return(review_data)
} else {
url_list <- vector(mode = "character", length = page_num)
for (i in 1:page_num){
if(i>1){
url_list[i] <- paste(substr(x=url,start=1,stop=(nchar(url)-4)), "_P",as.character(i),".htm", sep="")
} else{
url_list[i] <- url
}
}
counter <- 1
review_data <- vector(mode = "character", length = page_num*10)
for (y in 1:page_num){
url <- url_list[y]
webpage <- url %>% read_html()
reviews <- webpage %>% html_nodes( ".reviewBodyCell") %>% html_text()
for(x in 1:length(reviews)){
reviews <- gsub("Pros", " ", reviews)
reviews <- gsub("Cons", " ", reviews)
current <- substr(x=reviews[x],start=1, stop = regexpr('Share on Facebook', reviews[x])[1]-1)
review_data[counter] <- current
counter <- counter + 1
#print(paste(current))
}
}
return(review_data)
}
}
review_data <- getReviews(url="https://www.glassdoor.com/Reviews/U-S-Bank-Reviews-E8937.htm",page_num = 20)
con<-file('~/Documents/Junior_Year/DataAnalytics/Manhattan/US_bank.csv',encoding="UTF-8")
write.csv(review_data, file = con)
library(rvest)
getReviews <- function(url="https://www.glassdoor.com/Reviews/Veolia-Reviews-E20114.htm", page_num=1){
if (page_num <= 1){
webpage <- url %>% read_html()
reviews <- webpage %>% html_nodes( ".reviewBodyCell") %>% html_text()
review_data <- vector(mode = "character", length = length(reviews))
for(x in 1:length(reviews)){
reviews <- gsub("Pros", " ", reviews)
reviews <- gsub("Cons", " ", reviews)
current <- substr(x=reviews[x],start=1, stop = regexpr('Share on Facebook', reviews[x])[1]-1)
review_data[x] <- current
print(paste(current))
}
return(review_data)
} else {
url_list <- vector(mode = "character", length = page_num)
for (i in 1:page_num){
if(i>1){
url_list[i] <- paste(substr(x=url,start=1,stop=(nchar(url)-4)), "_P",as.character(i),".htm", sep="")
} else{
url_list[i] <- url
}
}
counter <- 1
review_data <- vector(mode = "character", length = page_num*10)
for (y in 1:page_num){
url <- url_list[y]
webpage <- url %>% read_html()
reviews <- webpage %>% html_nodes( ".reviewBodyCell") %>% html_text()
for(x in 1:length(reviews)){
reviews <- gsub("Pros", " ", reviews)
reviews <- gsub("Cons", " ", reviews)
reviews <- gsub("Advice to Management", " ", reviews)
current <- substr(x=reviews[x],start=1, stop = regexpr('Share on Facebook', reviews[x])[1]-1)
review_data[counter] <- current
counter <- counter + 1
#print(paste(current))
}
}
return(review_data)
}
}
review_data <- getReviews(url="https://www.glassdoor.com/Reviews/U-S-Bank-Reviews-E8937.htm",page_num = 430)
con<-file('~/Documents/Junior_Year/DataAnalytics/Manhattan/US_bank.csv',encoding="UTF-8")
write.csv(review_data, file = con)
review_data <- getReviews(url="https://www.glassdoor.com/Reviews/Transdev-Reviews-E413452.htm",page_num = 24)
con<-file('~/Documents/Junior_Year/DataAnalytics/Manhattan/Transdev.csv',encoding="UTF-8")
write.csv(review_data, file = con)
review_data <- getReviews(url="https://www.glassdoor.com/Reviews/First-Student-Reviews-E16694.htm",page_num = 44)
review_data <- getReviews(url="https://www.glassdoor.com/Reviews/First-Student-Reviews-E16694.htm",page_num = 44)
con<-file('~/Documents/Junior_Year/DataAnalytics/Manhattan/First_student.csv',encoding="UTF-8")
write.csv(review_data, file = con)
review_data <- getReviews(url="https://www.glassdoor.com/Reviews/Veolia-Reviews-E20114.htm",page_num = 63)
con<-file('~/Documents/Junior_Year/DataAnalytics/Manhattan/Veolia.csv',encoding="UTF-8")
write.csv(review_data, file = con)
review_data <- getReviews(url="https://www.glassdoor.com/Reviews/Harvard-Pilgrim-Reviews-E2816.htm",page_num = 10)
con<-file('~/Documents/Junior_Year/DataAnalytics/Manhattan/Harvard_pilgrim.csv',encoding="UTF-8")
write.csv(review_data, file = con)
review_data <- getReviews(url="https://www.glassdoor.com/Reviews/TD-Reviews-E3767.htm",page_num = 144)
con<-file('~/Documents/Junior_Year/DataAnalytics/Manhattan/TD_bank.csv',encoding="UTF-8")
write.csv(review_data, file = con)
review_data <- getReviews(url="https://www.glassdoor.com/Reviews/Motorola-Solutions-Reviews-E427189.htm",page_num = 68)
con<-file('~/Documents/Junior_Year/DataAnalytics/Manhattan/Motorola.csv',encoding="UTF-8")
write.csv(review_data, file = con)
install.packages("gjam")
library(gjam)
install.packages("gjam")
install.packages("RcppArmadillo")
install.packages(gjam)
install.packages("gjam")
install.packages("RcppArmadillo")
install.packages("gjam")
library(gjam)
install.packages("coda")
install.packages("gridExtra")
install.packages('mgcv')
install.packages('zoo')
install.packages('Matrix')
install.packages('cowplot')
install.packages('spBayes')
install.packages('snow')
install.packages('snowfall')
install.packages('rlecuyer')
range(3,1)
diff(3,1)
25-21-1
25-21+1
l <- vector("list", 3)
l[[1]] <- 7
setwd("~/Documents/Junior_Year/DISC_REU/DISC_chesapeake/")
load(file='training_set.rda')
set.seed(2000)
set.seed(2000)
curr.species = 'scam'
current <- training[,which(names(training)==curr.species)]
samp <- sample(nrow(training), .6 * nrow(training))
train <- training[samp,]
test <- training[-samp,]
model = randomForest(scam ~ .,data = train[,-c(1:3,5:11,17)], keep.forest=TRUE)
model
setwd("~/Documents/Junior_Year/DISC_REU/DISC_chesapeake/")
load(file='training_set.rda')
set.seed(2000)
curr.species = 'scam'
current <- training[,which(names(training)==curr.species)]
samp <- sample(nrow(training), .6 * nrow(training))
train <- training[samp,]
test <- training[-samp,]
model = randomForest(scam ~ .,data = train[,-c(1:3,5:11,17)], keep.forest=TRUE)
library(ggplot2)
library(randomForest)
library(gridExtra)
library(reprtree)
model = randomForest(scam ~ .,data = train[,-c(1:3,5:11,17)], keep.forest=TRUE)
model
par(bg = '#ecf0f8f9')
plot(0, 0, type="n", ann=FALSE, axes=FALSE)
u <- par("usr") # The coordinates of the plot area
rect(u[1], u[3], u[2], u[4], col="white", border=NA)
par(new=TRUE)
plot(model, main="", cex.main=2, cex.lab=1.4, cex.axis=1.4)
#par(bg = '#ecf0f8f9')
#plot(0, 0, type="n", ann=FALSE, axes=FALSE)
#u <- par("usr") # The coordinates of the plot area
#rect(u[1], u[3], u[2], u[4], col="white", border=NA)
#par(new=TRUE)
plot(model, main="", cex.main=2, cex.lab=1.4, cex.axis=1.4)
par(bg = 'white')
#plot(0, 0, type="n", ann=FALSE, axes=FALSE)
#u <- par("usr") # The coordinates of the plot area
#rect(u[1], u[3], u[2], u[4], col="white", border=NA)
#par(new=TRUE)
plot(model, main="", cex.main=2, cex.lab=1.4, cex.axis=1.4)
#par(bg = '#ecf0f8f9')
#plot(0, 0, type="n", ann=FALSE, axes=FALSE)
#u <- par("usr") # The coordinates of the plot area
#rect(u[1] + 0.07, u[3], u[2], u[4], col="white", border=NA)
#par(new=TRUE)
varImpPlot(model, main="", cex.main=2, cex.lab=1.4, cex.axis=1.4)
library(raster)
library(magrittr)
library(rgdal)
library(ggplot2)
setwd("~/Documents/Junior_Year/DISC_REU/DISC_chesapeake/")
# create our crop region layer
e <- as(extent(365375, 366400, 4303600, 4304800), 'SpatialPolygons')
crs(e) <- "+proj=utm +zone=18"
# import bands, crop them to the SERC region and raster to a data frame
band2 <- "Landsat8/LC08_L1TP_015033_20160718_20170222_01_T1/LC08_L1TP_015033_20160718_20170222_01_T1_B2.TIF" %>% raster() %>% crop(y = e) %>% rasterToPoints()
band3 <- "Landsat8/LC08_L1TP_015033_20160718_20170222_01_T1/LC08_L1TP_015033_20160718_20170222_01_T1_B3.TIF" %>% raster() %>% crop(y = e) %>% rasterToPoints()
band4 <- "Landsat8/LC08_L1TP_015033_20160718_20170222_01_T1/LC08_L1TP_015033_20160718_20170222_01_T1_B4.TIF" %>% raster() %>% crop(y = e) %>% rasterToPoints()
band5 <- "Landsat8/LC08_L1TP_015033_20160718_20170222_01_T1/LC08_L1TP_015033_20160718_20170222_01_T1_B5.TIF" %>% raster() %>% crop(y = e) %>% rasterToPoints()
# covariates for model and dataset construction
evi.value <- 2.5 * ((band5[1,3] - band4[1,3]) / (((band4[1,3] * 6) + band5[1,3]) - ((7.5 * band2[1,3]) + 1)))
ndvi.value <- ((band5[,3] - band4[,3]) / (band5[,3] + band4[,3]))
ndwi.value <- (band3[,3] - band5[,3]) / (band3[,3] + band5[,3])
savi.value <- (1.5 * ((band5[,3] - band4[,3]) / (band5[,3] + band4[,3] + 1.5)))
full.data <- data.frame(x = band2[,1],
y = band2[,2],
band2 = band2[,3],
band3 = band3[,3],
band4 = band4[,3],
band5 = band5[,3],
ndvi = ndvi.value,
evi = evi.value,
ndwi = ndwi.value,
savi = savi.value)
ggplot(data = full.data, aes(x=x, y=y)) + geom_point(aes(color = ndvi))
# Species Map Import
species <- read.csv("~/Documents/Junior_Year/DISC_REU/DISC_chesapeake/DominantSpPerPlot.csv")
# remove plots with no location, if any
species <- species[!is.na(species$easting),]
species <- species[!is.na(species$northing),]
species.map <- data.frame(species[2:9],
lat = long.lat.coor.serc$coords.x1,
lon = long.lat.coor.serc$coords.x2,
easting = utm.coor.serc$coords.x1,
northing = utm.coor.serc$coords.x2)
rm(species, long.lat.coor.serc, utm.coor.serc)
# Species Map Import
species <- read.csv("~/Documents/Junior_Year/DISC_REU/DISC_chesapeake/DominantSpPerPlot.csv")
# remove plots with no location, if any
species <- species[!is.na(species$easting),]
species <- species[!is.na(species$northing),]
# convert UTM to lat, long
utm.coor.serc <- SpatialPoints(cbind(species$easting,species$northing),
proj4string=CRS("+proj=utm +zone=18"))
# Convert to lat/long
long.lat.coor.serc <- as.data.frame(spTransform(utm.coor.serc,CRS("+proj=longlat")))
species.map <- data.frame(species[2:9],
lat = long.lat.coor.serc$coords.x1,
lon = long.lat.coor.serc$coords.x2,
easting = utm.coor.serc$coords.x1,
northing = utm.coor.serc$coords.x2)
rm(species, long.lat.coor.serc, utm.coor.serc)
training <- data.frame(plot.id = numeric(521), easting = numeric(521), northing = numeric(521),
scam = numeric(521), ivfr = numeric(521), c4 = numeric(521), phau = numeric(521),
spcy = numeric(521), tyla = numeric(521), dead= numeric(521), bare_water = numeric(521),
band2 = numeric(521), band3 = numeric(521), band4 = numeric(521), band5 = numeric(521),
ndvi = numeric(521), evi = numeric(521), ndwi = numeric(521), savi = numeric(521))
count <- 0
for(i in 1:nrow(full.data)){
for (j in 1:nrow(species.map)){
if (species.map[j,11] - full.data[i,1] < 30 & species.map[j,11] - full.data[i,1] > 0 &
species.map[j,12] - full.data[i,2] < 30 & species.map[j,12] - full.data[i,2] > 0){
count = count + 1
training$plot.id[count] <- i
training[count,-c(1)] <- c(full.data[i,1:2],species.map[j,1:8], full.data[i,3:10])
}
}
}
paste("Yes count:", count)
nrow(training)
# reassign ordinal values words
for (z in 4:11){
training[which(is.na(training[,z])),z] <- "0%"
training[which(training[,z] == 0),z] <- "less than 1%"
training[which(training[,z] == 1),z] <- "1 - 5%"
training[which(training[,z] == 2),z] <- "6% - 25%"
training[which(training[,z] == 3),z] <- "26 - 50%"
training[which(training[,z] == 4),z] <- "51 - 75%"
training[which(training[,z] == 5),z] <- "76% - 100%"
training[,z] <- as.factor(training[,z])
}
#graph current species under landsat plots
ggplot() +
geom_rect(data=species.map, aes(xmin=(easting - min(training$easting))/30, xmax=(easting - min(training$easting) + 10)/30, ymin=(northing -min(training$northing))/30, ymax=(northing -min(training$northing) + 10)/30, fill = as.factor(unlist(species.map[2]))), color=NA) +
labs(title=paste("Population Abundance"), x="X (30m increment)", y="Y (30m increment)", fill = "Cover") +
geom_rect(data=training, aes(xmin=(easting - min(training$easting))/30, xmax=(easting - min(training$easting) + 30)/30, ymin=(northing-min(training$northing))/30, ymax=(northing + 30 - min(training$northing))/30), color="black", fill = NA)
for (sp in 1:8){
species.map[which(is.na(species.map[,sp])),sp] <- "0%"
species.map[which(species.map[,sp] == 0),sp] <- "less than 1%"
species.map[which(species.map[,sp] == 1),sp] <- "1 - 5%"
species.map[which(species.map[,sp] == 2),sp] <- "6% - 25%"
species.map[which(species.map[,sp] == 3),sp] <- "26 - 50%"
species.map[which(species.map[,sp] == 4),sp] <- "51 - 75%"
species.map[which(species.map[,sp] == 5),sp] <- "76% - 100%"
species.map[,sp] <- as.factor(species.map[,sp])
}
species.map$scam <- factor(species.map$scam, levels = c("0%", "less than 1%", "1 - 5%", "6% - 25%", "26 - 50%", "51 - 75%", "76% - 100%"))
# blue scale for scam
cols = c("0%"="#ceb467", "less than 1%" = "#d1ddfc", "1 - 5%" = "#adc0f4", "6% - 25%" = "#8ba6f4", "26 - 50%" = "#5e82ed", "51 - 75%" = "#2c2ccc", "76% - 100%" = "#231572")
serc.plots <- ggplot() +
geom_rect(data=species.map, aes(xmin=easting, xmax=easting + 10, ymin=northing, ymax=northing + 10, fill = as.factor(unlist(species.map[1]))), color=NA) +
labs(title=paste("Schoenoplectus Population Abundance in SERC Plots"), x="Easting", y="Northing", fill = "Cover") +
geom_rect(data=training, aes(xmin=easting, xmax=easting + 30, ymin=northing, ymax=northing + 30), color="black", fill = NA) +
scale_x_continuous(limits = c(365430, 366280)) +
scale_y_continuous(limits = c(4303800, 4304470)) +
scale_fill_manual(values = cols) +
theme(plot.background = element_rect(fill = '#ecf0f8f9'),
axis.line=element_blank(),
axis.text.x=element_blank(),
axis.text.y=element_blank(),
axis.ticks=element_blank(),
axis.title.x=element_blank(),
axis.title.y=element_blank())
serc.plots
serc.plots <- ggplot() +
geom_rect(data=species.map, aes(xmin=easting, xmax=easting + 10, ymin=northing, ymax=northing + 10, fill = as.factor(unlist(species.map[1]))), color=NA) +
labs(title=paste("Schoenoplectus Population Abundance in SERC Plots"), x="Easting", y="Northing", fill = "Cover") +
geom_rect(data=training, aes(xmin=easting, xmax=easting + 30, ymin=northing, ymax=northing + 30), color="black", fill = NA) +
scale_x_continuous(limits = c(365430, 366280)) +
scale_y_continuous(limits = c(4303800, 4304470)) +
scale_fill_manual(values = cols) +
theme(plot.background = element_rect(fill = 'white'),
axis.line=element_blank(),
axis.text.x=element_blank(),
axis.text.y=element_blank(),
axis.ticks=element_blank(),
axis.title.x=element_blank(),
axis.title.y=element_blank())
serc.plots
pred <- predict(model, newdata = test)
table(pred, test$scam)
pred <- factor(pred, levels = c("0%", "less than 1%", "1 - 5%", "6% - 25%", "26 - 50%", "51 - 75%", "76% - 100%"))
cols = c("0%"="#ceb467", "less than 1%" = "#ace5b2", "1 - 5%" = "#7aef87", "6% - 25%" = "#57d165", "26 - 50%" = "#21a31d", "51 - 75%" = "#197f24", "76% - 100%" = "#115118")
pred.graph <- ggplot() +
geom_rect(data=test, aes(xmin=easting, xmax=easting + 30, ymin=northing, ymax=northing + 30, fill = as.factor(unlist(pred))), color="black") +
labs(title=paste("Pop. Abundance Prediction on 40% of Plots"), x="Easting", y="Northing", fill="Cover") +
scale_x_continuous(limits = c(365430, 366280)) +
scale_y_continuous(limits = c(4303800, 4304470)) +
scale_fill_manual(values = cols)
pred.graph
pred2 <- predict(model, newdata = training)
table(pred2, training$scam)
pred2 <- factor(pred2, levels = c("0%", "less than 1%", "1 - 5%", "6% - 25%", "26 - 50%", "51 - 75%", "76% - 100%"))
# blue scale for scam
cols = c("0%"="#ceb467", "less than 1%" = "#d1ddfc", "1 - 5%" = "#adc0f4", "6% - 25%" = "#8ba6f4", "26 - 50%" = "#5e82ed", "51 - 75%" = "#2c2ccc", "76% - 100%" = "#231572")
pred2.graph <- ggplot() +
geom_rect(data=training, aes(xmin=easting, xmax=easting + 30, ymin=northing, ymax=northing + 30, fill = as.factor(unlist(pred2))), color="black") +
labs(title=paste("Schoenoplectus Population Classifier Predictions"),x="Easting", y="Northing", fill="Cover") +
scale_x_continuous(limits = c(365430, 366280)) +
scale_y_continuous(limits = c(4303800, 4304470)) +
scale_fill_manual(values = cols) + theme(plot.background = element_rect(fill = 'white'),
axis.line=element_blank(),
axis.text.x=element_blank(),
axis.text.y=element_blank(),
axis.ticks=element_blank(),
axis.title.x=element_blank(),
axis.title.y=element_blank())
pred2.graph
# need to run training.R concurrently
grid.arrange(serc.plots, pred2.graph, ncol=2)
species.map$phau <- factor(species.map$phau, levels = c("0%", "less than 1%", "1 - 5%", "6% - 25%", "26 - 50%", "51 - 75%", "76% - 100%"))
# red scale for Phrag
cols = c("0%"="#ceb467", "less than 1%" = "#fce0e0", "1 - 5%" = "#ffa8a8", "6% - 25%" = "#f26a6a", "26 - 50%" = "#e83e3e", "51 - 75%" = "#c12020", "76% - 100%" = "#891818")
serc.plots <- ggplot() +
geom_rect(data=species.map, aes(xmin=easting, xmax=easting + 10, ymin=northing, ymax=northing + 10, fill = as.factor(unlist(species.map[4]))), color=NA) +
labs(title=paste("Schoenoplectus Population Abundance in SERC Plots"), x="Easting", y="Northing", fill = "Cover") +
geom_rect(data=training, aes(xmin=easting, xmax=easting + 30, ymin=northing, ymax=northing + 30), color="black", fill = NA) +
scale_x_continuous(limits = c(365430, 366280)) +
scale_y_continuous(limits = c(4303800, 4304470)) +
scale_fill_manual(values = cols) +
theme(plot.background = element_rect(fill = 'white'),
axis.line=element_blank(),
axis.text.x=element_blank(),
axis.text.y=element_blank(),
axis.ticks=element_blank(),
axis.title.x=element_blank(),
axis.title.y=element_blank())
serc.plots
serc.plots <- ggplot() +
geom_rect(data=species.map, aes(xmin=easting, xmax=easting + 10, ymin=northing, ymax=northing + 10, fill = as.factor(unlist(species.map[4]))), color=NA) +
labs(title=paste("Phragmites Population Abundance in SERC Plots"), x="Easting", y="Northing", fill = "Cover") +
geom_rect(data=training, aes(xmin=easting, xmax=easting + 30, ymin=northing, ymax=northing + 30), color="black", fill = NA) +
scale_x_continuous(limits = c(365430, 366280)) +
scale_y_continuous(limits = c(4303800, 4304470)) +
scale_fill_manual(values = cols) +
theme(plot.background = element_rect(fill = 'white'),
axis.line=element_blank(),
axis.text.x=element_blank(),
axis.text.y=element_blank(),
axis.ticks=element_blank(),
axis.title.x=element_blank(),
axis.title.y=element_blank())
serc.plots
setwd("~/Documents/Junior_Year/DISC_REU/DISC_chesapeake/")
load(file='training_set.rda')
set.seed(2000)
curr.species = 'phau'
current <- training[,which(names(training)==curr.species)]
samp <- sample(nrow(training), .6 * nrow(training))
train <- training[samp,]
test <- training[-samp,]
model = randomForest(phau ~ .,data = train[,-c(1:6,8:11,17)], keep.forest=TRUE)
model
#par(bg = '#ecf0f8f9')
#plot(0, 0, type="n", ann=FALSE, axes=FALSE)
#u <- par("usr") # The coordinates of the plot area
#rect(u[1], u[3], u[2], u[4], col="white", border=NA)
#par(new=TRUE)
plot(model, main="", cex.main=2, cex.lab=1.4, cex.axis=1.4)
pred <- predict(model, newdata = test)
table(pred, test$scam)
pred <- factor(pred, levels = c("0%", "less than 1%", "1 - 5%", "6% - 25%", "26 - 50%", "51 - 75%", "76% - 100%"))
cols = c("0%"="#ceb467", "less than 1%" = "#ace5b2", "1 - 5%" = "#7aef87", "6% - 25%" = "#57d165", "26 - 50%" = "#21a31d", "51 - 75%" = "#197f24", "76% - 100%" = "#115118")
pred.graph <- ggplot() +
geom_rect(data=test, aes(xmin=easting, xmax=easting + 30, ymin=northing, ymax=northing + 30, fill = as.factor(unlist(pred))), color="black") +
labs(title=paste("Pop. Abundance Prediction on 40% of Plots"), x="Easting", y="Northing", fill="Cover") +
scale_x_continuous(limits = c(365430, 366280)) +
scale_y_continuous(limits = c(4303800, 4304470)) +
scale_fill_manual(values = cols)
pred.graph
pred2 <- predict(model, newdata = training)
table(pred2, training$scam)
pred2 <- factor(pred2, levels = c("0%", "less than 1%", "1 - 5%", "6% - 25%", "26 - 50%", "51 - 75%", "76% - 100%"))
# blue scale for scam
cols = c("0%"="#ceb467", "less than 1%" = "#d1ddfc", "1 - 5%" = "#adc0f4", "6% - 25%" = "#8ba6f4", "26 - 50%" = "#5e82ed", "51 - 75%" = "#2c2ccc", "76% - 100%" = "#231572")
# red scale for Phrag
cols = c("0%"="#ceb467", "less than 1%" = "#fce0e0", "1 - 5%" = "#ffa8a8", "6% - 25%" = "#f26a6a", "26 - 50%" = "#e83e3e", "51 - 75%" = "#c12020", "76% - 100%" = "#891818")
pred <- predict(model, newdata = test)
table(pred, test$phau)
pred <- factor(pred, levels = c("0%", "less than 1%", "1 - 5%", "6% - 25%", "26 - 50%", "51 - 75%", "76% - 100%"))
cols = c("0%"="#ceb467", "less than 1%" = "#ace5b2", "1 - 5%" = "#7aef87", "6% - 25%" = "#57d165", "26 - 50%" = "#21a31d", "51 - 75%" = "#197f24", "76% - 100%" = "#115118")
pred.graph <- ggplot() +
geom_rect(data=test, aes(xmin=easting, xmax=easting + 30, ymin=northing, ymax=northing + 30, fill = as.factor(unlist(pred))), color="black") +
labs(title=paste("Pop. Abundance Prediction on 40% of Plots"), x="Easting", y="Northing", fill="Cover") +
scale_x_continuous(limits = c(365430, 366280)) +
scale_y_continuous(limits = c(4303800, 4304470)) +
scale_fill_manual(values = cols)
pred.graph
pred2 <- predict(model, newdata = training)
table(pred2, training$phau)
pred2 <- factor(pred2, levels = c("0%", "less than 1%", "1 - 5%", "6% - 25%", "26 - 50%", "51 - 75%", "76% - 100%"))
# red scale for Phrag
cols = c("0%"="#ceb467", "less than 1%" = "#fce0e0", "1 - 5%" = "#ffa8a8", "6% - 25%" = "#f26a6a", "26 - 50%" = "#e83e3e", "51 - 75%" = "#c12020", "76% - 100%" = "#891818")
pred2.graph <- ggplot() +
geom_rect(data=training, aes(xmin=easting, xmax=easting + 30, ymin=northing, ymax=northing + 30, fill = as.factor(unlist(pred2))), color="black") +
labs(title=paste("Phragmites Population Classifier Predictions"),x="Easting", y="Northing", fill="Cover") +
scale_x_continuous(limits = c(365430, 366280)) +
scale_y_continuous(limits = c(4303800, 4304470)) +
scale_fill_manual(values = cols) + theme(plot.background = element_rect(fill = 'white'),
axis.line=element_blank(),
axis.text.x=element_blank(),
axis.text.y=element_blank(),
axis.ticks=element_blank(),
axis.title.x=element_blank(),
axis.title.y=element_blank())
pred2.graph
# need to run training.R concurrently
grid.arrange(serc.plots, pred2.graph, ncol=2)
